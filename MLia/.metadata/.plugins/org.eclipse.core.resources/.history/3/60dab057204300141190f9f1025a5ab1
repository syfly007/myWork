from math import log
import operator

def calcShannonEnt(dataSet):
	numEntries = len(dataSet)
	labelCounts = {}
	for featVec in dataSet:
		currentLable = featVec [-1]
		if currentLable not in labelCounts.keys():
			labelCounts[currentLable] = 0
		labelCounts[currentLable] += 1
	shannonEnt = 0.0
	for key in labelCounts:
		prob = float(labelCounts[key])/numEntries
		shannonEnt -= prob*log(prob,2)
	return shannonEnt

def createDataSet():
	dataSet = [
		[1,1,'yes'],
		[1,1,'yes'],
		[1,0,'no'],
		[0,1,'no'],
		[0,1,'no']
	]
	labels = ['no surfacing','flippers']
	return dataSet,labels

def splitDataSet(dataSet,axis,value):
	retDataSet = []
	for featVec in dataSet:
		if featVec[axis] == value:
			reducedFeatVec = featVec[:axis]
			reducedFeatVec.extend(featVec[axis+1:])
			retDataSet.append(reducedFeatVec)
	return retDataSet

def chooseBestFeatureToSplit(dataSet):
	featureNum = len(dataSet[0]) - 1 #the last column is used for the result labels
	baseEntropy = calcShannonEnt(dataSet)
	bestInfoGain = 0.0
	bestFeature = -1
	for i in range(featureNum):
		featureList = [example[i] for example in dataSet]
		uniqueVals = set(featureList) ##remove duplicate
		newEntropy = 0.0
		for value in uniqueVals:
			subDataSet = splitDataSet(dataSet,i,value)
			prob = float(len(subDataSet))/len(dataSet)
			newEntropy += prob*calcShannonEnt(subDataSet)
			infoGain = baseEntropy - newEntropy## how to prove every newEntropy < baseEntropy?
			if infoGain > bestInfoGain:
				bestInfoGain = infoGain
				bestFeature = i
	return bestFeature

def majorityCnt(classList):
	classCount = {}
	for vote in classList:
		if vote not in classCount.keys():
			classCount[vote] = 0
		classCount[vote] += 1
	sortedClassCount = sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)  
	return


# test
myDat,labels = createDataSet()
print myDat
# print calcShannonEnt(myDat)
# print splitDataSet(myDat,1,1)
# print splitDataSet(myDat,0,0)
print chooseBestFeatureToSplit(myDat)